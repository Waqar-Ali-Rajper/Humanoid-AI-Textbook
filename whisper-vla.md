# whisper-vla

Title: Whisper and Vision Language Action

You link language, vision, and action. The robot listens, understands, and performs a task.

Speech input

Whisper converts voice to text

Your controller reads the text

The LLM interprets user intent

Vision

Camera provides RGB

Depth sensor gives distance

Model finds objects

VLA pipeline combines language and vision

Action

LLM outputs steps

ROS 2 sends commands to motors

Robot follows sequence safely

Example
User says: “Pick the cup from the table.”

Pipeline

Whisper transcribes

LLM identifies goal

Vision finds cup

Planner generates motion

Controller executes the pick

This chapter prepares you for the final project.
